inner_join(completed)
head(hm_data)
# Step 1 - Load the processed text data along with demographic information on contributors
# We use the processed data for our analysis and combine it with the demographic information available.
hm_data <- read_csv("../output/processed_moments.csv")
write_csv(hm_data, "../output/processed_moments.csv")
# We use the processed data for our analysis and combine it with the demographic information available.
hm_data <- read_csv("../output/processed_moments.csv")
head(hm_data)
## What I want to do now: subset df -> merge hm_data & demo_data
head(demo_data)
head(demo_data)
head(hm_data)
colnames(demo_data)
colnames(hm_data)
head(hm_data)
#colnames(demo_data)
```
head(demo_data)
colnames(hm_data)
#
colnames(demo_data)
#colnames(hm_data)
```
head(hm_data)
## What I want to do now: subset df -> merge hm_data & demo_data
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
age,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
```
## What I want to do now: subset df -> merge hm_data & demo_data
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
age,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
```
```{r, message=FALSE,echo=FALSE}
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
## What I want to do now: subset df -> merge hm_data & demo_data
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
age,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
## What I want to do now: subset df -> merge hm_data & demo_data
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
age,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
hm_data
## What I want to do now: subset df -> merge hm_data & demo_data
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
age,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
datatalbe(hm_data)
## What I want to do now: subset df -> merge hm_data & demo_data
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
age,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
datatable(hm_data)
```{r, message=FALSE,echo=FALSE}
datatable(hm_data)
datatable(hm_data)
```{r, message=FALSE,echo=FALSE}
# Step 1 - Load the processed text data along with demographic information on contributors
# We use the processed data for our analysis and combine it with the demographic information available.
hm_data <- read_csv("../output/processed_moments.csv")
head(hm_data)
```
# We use the processed data for our analysis and combine it with the demographic information available.
hm_data <- read_csv("../output/processed_moments.csv")
head(hm_data)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
## What I want to do now: subset df -> merge hm_data & demo_data
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
age,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
```
```{r, message=FALSE,echo=FALSE}
datatable(hm_data)
hm_data
colnames(hm_data)
write.csv(hm_data, "/Users/jasoncho/Downloads/P1\\hm_data.csv", row.names=FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
#head(hm_data, 10)
dim(hm_data)
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
stemmed <- tm_map(corpus, stemDocument) %>%
tidy() %>%
select(text)
dict <- tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
data("stop_words")
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past")
stop_words <- stop_words %>%
bind_rows(mutate(tibble(word), lexicon = "updated"))
completed <- stemmed %>%
mutate(id = row_number()) %>%
unnest_tokens(stems, text) %>%
bind_cols(dict) %>%
anti_join(stop_words, by = c("dictionary" = "word"))
completed <- completed %>%
group_by(stems) %>%
count(dictionary) %>%
mutate(word = dictionary[which.max(n)]) %>%
ungroup() %>%
select(stems, word) %>%
distinct() %>%
right_join(completed) %>%
select(-stems)
completed <- completed %>%
group_by(id) %>%
summarise(text = str_c(word, collapse = " ")) %>%
ungroup()
hm_data <- hm_data %>%
mutate(id = row_number()) %>%
inner_join(completed)
head(hm_data)
write_csv(hm_data, "../output/processed_moments.csv")
write.csv(hm_data, "/Users/jasoncho/Downloads/P1\\hm_data.csv", row.names=FALSE)
# Step 1 - Load the processed text data along with demographic information on contributors
# We use the processed data for our analysis and combine it with the demographic information available.
hm_data <- read_csv("../output/processed_moments.csv")
head(hm_data)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
## What I want to do now: subset df -> merge hm_data & demo_data
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
age,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
dim(hm_data)
knitr::opts_chunk$set(echo = TRUE)
if (!requireNamespace("tidytext", quietly = TRUE)) {
install.packages("tidytext")
}
library(magrittr)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(dplyr)
library(ggplot2)
library(wordcloud)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, stopwords("en"))%>%
tm_map(stripWhitespace)%>%
tm_map(stemDocument)
stemmed <- tm_map(corpus, stemDocument) %>%
tidy() %>%
select(text)
dict <- corpus %>%
tidy() %>%
unnest_tokens(dictionary, text) %>%
select(dictionary)
data("stop_words")
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past","and",
"but","with","in","on","oh","wow","ah","day", "happi")
stop_words <- stop_words %>%
bind_rows(mutate(tibble(word), lexicon = "updated"))
completed <- stemmed %>%
mutate(id = row_number()) %>%
unnest_tokens(stems, text) %>%
bind_cols(dict) %>%
anti_join(stop_words, by = c("dictionary" = "word"))
completed <- completed %>%
group_by(stems) %>%
count(dictionary) %>%
mutate(word = dictionary[which.max(n)]) %>%
ungroup() %>%
select(stems, word) %>%
distinct() %>%
right_join(completed) %>%
select(-stems)
completed <- completed %>%
group_by(id) %>%
summarise(text = str_c(word, collapse = " ")) %>%
ungroup()
hm_data <- hm_data %>%
mutate(id = row_number()) %>%
inner_join(completed)
write_csv(hm_data, "../output/processed_moments.csv")
hm_data <- read_csv("../output/processed_moments.csv")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
# remove rows with NA value
combined_data <- merge(hm_data,demo_data, by="wid", all.x=TRUE, all.y=TRUE)
combined_data <- na.omit(combined_data)
# Subset the desired columns and name it as 'subset_data'
subset_data <- combined_data[, c("wid", "age", "gender", "original_hm", "text", "modified", "ground_truth_category", "predicted_category", "num_sentence")]
subset_data <- subset(subset_data, age >= 5 & age <= 116)
# Count the number of responses for each age
age_counts <- table(subset_data$age)
age_df <- as.data.frame(age_counts)
colnames(age_df) <- c('Age', 'Count')
# Convert Age to numeric for plotting
age_df$Age <- as.numeric(as.character(age_df$Age))
# Plotting the number of responses across ages
ggplot(age_df, aes(x=Age, y=Count)) +
geom_line(color='black') +
theme_minimal() +
labs(title='Number of Responses Across Ages',
x='Age',
y='Number of Responses') +
theme(axis.text.x = element_text(angle=45, hjust=1))
# Add a new column for age groups
subset_data$age_group <- cut(subset_data$age,
breaks = c(0, 10, 20, 30, 40, 50, 60, Inf),
labels = c("0-9", "10-19", "20-29", "30-39", "40-49", "50-59", "60 +"),
right = TRUE, include.lowest = TRUE)
# Since the minimum age of the data is 18, we will not be getting any data for '0-9' group
min(unique(subset_data$age))
# Plotting the distribution of moments across age groups
ggplot(subset_data, aes(x=age_group, fill=ground_truth_category)) +
geom_bar(position='dodge') +
theme_minimal() +
labs(title='Distribution of Moments Across Age Groups',
x='Age Group',
y='Count') +
theme(axis.text.x = element_text(angle=45, hjust=1),
legend.position='top',
legend.title=element_blank())
# Plotting the distribution of moments across age groups as percentages
ggplot(subset_data, aes(x=age_group, fill=ground_truth_category)) +
geom_bar(position='fill') +
scale_y_continuous(labels=scales::percent_format(scale=1)) +
theme_minimal() +
labs(title='Percentage Distribution of Moments Across Age Groups in Bar Plot',
x='Age Group',
y='Percentage') +
theme(axis.text.x = element_text(angle=45, hjust=1),
legend.position='top',
legend.title=element_blank())
# Calculate the percentages for each category within each age group
subset_data_percent <- subset_data %>%
group_by(age_group, ground_truth_category) %>%
summarise(count = n()) %>%
mutate(percentage = count / sum(count))
# Plot the data using geom_line
ggplot(subset_data_percent, aes(x=age_group, y=percentage, color=ground_truth_category, group=ground_truth_category)) +
geom_line(size=1) +
scale_y_continuous(labels=scales::percent_format(scale=1)) +
theme_minimal() +
labs(title='Percentage Distribution of Moments Across Age Groups in Line Graph',
x='Age Group',
y='Percentage') +
theme(axis.text.x = element_text(angle=45, hjust=1),
legend.position='top',
legend.title=element_blank())
# Calculating the length of the response
subset_data$length_response <- nchar(as.character(subset_data$original_hm))
# Compute the average length of responses for each age group
avg_length_by_age <- subset_data %>%
group_by(age_group) %>%
summarise(avg_length = mean(length_response))
# Plot the average length of responses across age groups
ggplot(avg_length_by_age, aes(x=age_group, y=avg_length)) +
geom_col(fill="skyblue") +
theme_minimal() +
labs(title='Average Length of Responses Across Age Groups',
x='Age Group',
y='Average Length') +
theme(axis.text.x = element_text(angle=45, hjust=1))
# Scatterplot of the length of responses across all ages
ggplot(subset_data, aes(x=age, y=length_response)) +
geom_point(alpha=0.2, color="blue") +
theme_minimal() +
labs(title='Length of Responses Across All Ages',
x='Age',
y='Length of Response') +
theme(axis.text.x = element_text(angle=45, hjust=1))
# Wordcloud for each age group
# Subsetting data by age groups. Subsetted data will only contain agegroup and text columns
text_0_9 <- subset_data[subset_data$age_group == '0-9',]
text_10_19 <- subset_data[subset_data$age_group == '10-19',]
text_20_29 <- subset_data[subset_data$age_group == '20-29',]
text_30_39 <- subset_data[subset_data$age_group == '30-39',]
text_40_49 <- subset_data[subset_data$age_group == '40-49',]
text_50_59 <- subset_data[subset_data$age_group == '50-59',]
text_60 <- subset_data[subset_data$age_group == '60 +',]
text_data_0_9 <- text_0_9$text
text_data_10_19 <- text_10_19$text
text_data_20_29 <- text_20_29$text
text_data_30_39 <- text_30_39$text
text_data_40_49 <- text_40_49$text
text_data_50_59 <- text_50_59$text
text_data_60_69 <- text_60$text
# Wordcloud for age group 10-19
# Skipped group 0-9 since there were no values for that age group
bag_of_words_19 <-  text_10_19 %>%
unnest_tokens(word, text)
word_count_19 <- bag_of_words_19 %>%
count(word, sort = TRUE)
wordcloud(word_count_19$word,word_count_19$n ,
scale=c(3,0.1),
max.words=100,
min.freq=1,
random.order=FALSE,
rot.per=0.3,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"Oranges"))
title(main = "Word Cloud for Age Group 10-19")
# Wordcloud for age group 20-29
bag_of_words_29 <-  text_20_29 %>%
unnest_tokens(word, text)
word_count_29 <- bag_of_words_29 %>%
count(word, sort = TRUE)
wordcloud(word_count_29$word,word_count_29$n ,
scale=c(3,0.1),
max.words=100,
min.freq=1,
random.order=FALSE,
rot.per=0.3,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"Greens"))
title(main = "Word Cloud for Age Group 20-29")
# Wordcloud for age group 30-39
bag_of_words_39 <-  text_30_39 %>%
unnest_tokens(word, text)
word_count_39 <- bag_of_words_39 %>%
count(word, sort = TRUE)
wordcloud(word_count_39$word,word_count_39$n ,
scale=c(3,0.1),
max.words=100,
min.freq=1,
random.order=FALSE,
rot.per=0.3,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"Blues"))
title(main = "Word Cloud for Age Group 30-39")
# Wordcloud for age group 40-49
bag_of_words_49 <-  text_40_49 %>%
unnest_tokens(word, text)
word_count_49 <- bag_of_words_49 %>%
count(word, sort = TRUE)
wordcloud(word_count_49$word,word_count_49$n ,
scale=c(3,0.1),
max.words=100,
min.freq=1,
random.order=FALSE,
rot.per=0.3,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"RdPu"))
title(main = "Word Cloud for Age Group 40-49")
# Wordcloud for age group 50-59
bag_of_words_59 <-  text_50_59 %>%
unnest_tokens(word, text)
word_count_59 <- bag_of_words_59 %>%
count(word, sort = TRUE)
wordcloud(word_count_59$word,word_count_59$n ,
scale=c(3,0.1),
max.words=100,
min.freq=1,
random.order=FALSE,
rot.per=0.3,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"GnBu"))
title(main = "Word Cloud for Age Group 50-59")
# Wordcloud for age group 60+
bag_of_words_60 <-  text_60 %>%
unnest_tokens(word, text)
word_count_60 <- bag_of_words_60 %>%
count(word, sort = TRUE)
wordcloud(word_count_60$word,word_count_60$n ,
scale=c(3,0.1),
max.words=100,
min.freq=1,
random.order=FALSE,
rot.per=0.3,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"Purples"))
title(main = "Word Cloud for Age Group 60+")
install.packages("httr")
library(httr)
url <- "https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv"
dest_file <- "./data/cleaned_hm.csv"
GET(url, write_disk(dest_file, overwrite = TRUE))
desktop_path <- "~/Desktop"
dest_file <- file.path(desktop_path, "cleaned_hm.csv")
GET(url, write_disk(dest_file, overwrite = TRUE))
dest_file <- file.path(desktop_path, "demographic.csv")
GET(urlfile, write_disk(dest_file, overwrite = TRUE))
knitr::opts_chunk$set(echo = TRUE)
if (!requireNamespace("tidytext", quietly = TRUE)) {
install.packages("tidytext")
}
library(magrittr)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(dplyr)
library(ggplot2)
library(wordcloud)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
# Exporting the csv file
install.packages("httr")
library(httr)
desktop_path <- "~/Desktop"
url <- "https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv"
dest_file <- file.path(desktop_path, "cleaned_hm.csv")
GET(url, write_disk(dest_file, overwrite = TRUE))
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, stopwords("en"))%>%
tm_map(stripWhitespace)%>%
tm_map(stemDocument)
